# ğŸ¯ Multimodal Embedding Project

## ğŸ“Œ Overview
This project demonstrates how to build a **multimodal embedding pipeline** that integrates text, PDF documents, audio, and images into a unified vector space using **FAISS** for similarity search.  
It leverages **Sentence Transformers**, **OpenAI Whisper**, and **CLIP** to generate embeddings across modalities, enabling powerful search and retrieval.

---

## ğŸš€ Features
- **Text embeddings** using SentenceTransformer (`all-MiniLM-L6-v2`)
- **PDF parsing** with PyPDF2
- **Audio transcription** with Whisper
- **Image embeddings** with CLIP
- **Vector search** using FAISS
- Unified storage and retrieval of multimodal content

---

## ğŸ› ï¸ Installation
Clone the repository and install dependencies:

```bash
git clone https://github.com/Saloni-2428/multimodal-embedding-project.git
cd multimodal-embedding-project
pip install -r requirements.txt


---

## ğŸ§ª Usage / Example Code
You can run the main script (`train.py`) to add text, PDF, audio, and image embeddings into FAISS and perform semantic search.

### Quick Example
```python
from sentence_transformers import SentenceTransformer
import faiss

# Load text model
text_model = SentenceTransformer("all-MiniLM-L6-v2")

# Initialize FAISS index
dimension = 384
index = faiss.IndexFlatL2(dimension)
documents = []

def add_text(text, source):
    vec = text_model.encode([text]).astype("float32")
    index.add(vec)
    documents.append(f"{source}: {text[:80]}")

# Add a sample text
add_text("Artificial Intelligence is transforming healthcare.", "Text file")

# Search
def search(query, k=3):
    qvec = text_model.encode([query]).astype("float32")
    _, idx = index.search(qvec, k)
    return [documents[i] for i in idx[0]]

results = search("How is AI used in healthcare?")
print("ğŸ” Search Results:", results)




ğŸ“‚ Project Structure
multimodal-embedding-project/
â”‚â”€â”€ README.md                # Project documentation
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ train.py                 # Main script for embeddings & search
â”‚â”€â”€ data/                    # Sample data folder
â”‚   â”œâ”€â”€ sample_text.txt
â”‚   â”œâ”€â”€ sample_document.pdf
â”‚   â”œâ”€â”€ sample_audio.wav
â”‚   â””â”€â”€ sample_image.jpg
â”‚â”€â”€ utils/                   # Helper functions (optional)
â”‚   â””â”€â”€ preprocessing.py
â”‚â”€â”€ models/                  # Pretrained or fine-tuned models
â”‚â”€â”€ notebooks/               # Jupyter notebooks for experiments
â”‚   â””â”€â”€ demo.ipynb



ğŸ§  Models Used
SentenceTransformer: Text embeddings
Whisper: Audio transcription
CLIP: Image embeddings
FAISS: Vector similarity search


---

## ğŸ“Š Results
- Integrates **text, PDF, audio, and image embeddings** into a single FAISS index.
- Enables **semantic search** across modalities.
- Example query: *â€œHow is AI used in healthcare?â€* returns relevant snippets from text, PDF, audio transcription, and image content.

---

## ğŸ“š Datasets
You can experiment with:
- [COCO Dataset](https://cocodataset.org/)
- [Flickr30k Dataset](https://www.bing.com/search?q=Flickr30k+dataset)

---

## ğŸ¤ Contributing
Contributions are welcome!  
1. Fork the repo  
2. Create a new branch  
3. Commit changes  
4. Submit a pull request  

--

## ğŸ“¬ Contact
Created by **Saloni-2428**  
For questions or collaborations, reach out via [GitHub profile](https://github.com/Saloni-2428).
